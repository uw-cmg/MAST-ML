# You run this with
# python3 -m mastml.mastml_driver tests/conf/Diffusion_MLMR.conf tests/csv/Diffusion_MLMR.csv -o results/Diffusion_MLMR

[GeneralSetup]
    input_features = Auto
    target_feature = Reduced barrier (eV)
    randomizer = False
    metrics = Auto
    not_input_features = Host element, Solute element, predict_Pt, predict_Ag, predict_W
    #not_input_features = Atomic number_1, Material compositions 1, Material compositions 2
    grouping_feature = Host element
    #validation_columns = predict_Ag
    #validation_columns = predict_Pt, predict_Ag, predict_W

#[DataCleaning]
#    cleaning_method = remove
#    imputation_strategy = mean

#[Clustering]
#    [[KMeans_5Clusters]]
#        n_clusters = 5
#    [[SpectralClustering_14Clusters]]
#        n_clusters = 14

#[FeatureGeneration]
    #[[Magpie]]
    #    composition_feature = Material compositions
    #[[ContainsElement]]
    #    composition_feature = Host element
    #    all_elements = True
    #    element = Al # Ignored if all_elements = True
    #    new_name = has_Al # Ignored if all_elements = True

#[FeatureNormalization]
#    [[StandardScaler]]

#[LearningCurve]
#    estimator = KernelRidge_select
#    cv = RepeatedKFold_select
#    scoring = root_mean_squared_error
#    n_features_to_select = 20
#    selector_name = MASTMLFeatureSelector

#[FeatureSelection]
    #[[RFE]]
    #    estimator = RandomForestRegressor_select
    #    n_features_to_select = 20
    #[[SequentialFeatureSelector]]
    #     estimator = RandomForestRegressor
    #     k_features = 10
    #[[RFECV]]
    #     estimator = RandomForestRegressor_select
    #     step = 1
    #     cv = LeaveOneGroupOut_select
    #     #cv = RepeatedKFold_select
    #     #cv = KFold_select
    #[[MASTMLFeatureSelector]]
    #    estimator = KernelRidge_select
    #    n_features_to_select = 5
    #    #cv = KFold_select
    #    cv = LeaveOneGroupOut_select
    #[[SelectKBest]]

#[DataSplits]
    #[[NoSplit]]
    #[[KFold_select]]
    #    shuffle = True
    #    n_splits = 10
    #[[RepeatedKFold_select]]
    #    n_splits = 5
    #    n_repeats = 2
    #[[RepeatedKFold]]
    #    n_splits = 3
    #    n_repeats = 2
    #[[LeaveOneGroupOut_select]]
    #    grouping_column = Host element
    #[[LeaveOneGroupOut_learning]]
    #    grouping_column = Host element
    #[[LeaveOneGroupOut_Al]]
    #    grouping_column = has_Al
    #[[LeaveOneGroupOut_Zr]]
    #    grouping_column = has_Zr
    #[[LeaveOneGroupOut_Mo]]
    #    grouping_column = has_Mo
    #[[LeaveOneGroupOut_host]]
    #    grouping_column = Host element
    #[[LeaveOneGroupOut_kmeans]]
    #    grouping_column = KMeans_5Clusters
    #[[LeaveOneGroupOut_spectral]]
    #    grouping_column = SpectralClustering_14Clusters

#[Models]
	#[[LinearRegression]]
	#[[KernelRidge]]
    #	alpha = 0.009
	#	gamma = 0.027
	#	kernel = rbf
	#[[KernelRidge_select]]
    #	alpha = 0.009
	#	gamma = 0.027
	#	kernel = rbf
	#[[KernelRidge_learning]]
    #	alpha = 0.009
	#	gamma = 0.027
	#	kernel = rbf
	#[[RandomForestRegressor]]
	#    criterion = mse
	#    max_depth = 10
	#    max_leaf_nodes = 200
	#    min_samples_leaf = 1
	#    min_samples_split = 2
	#    n_estimators = 10
	#[[GaussianProcessRegressor]]
	#[[ExtraTreesRegressor_learning]]
	#[[RandomForestRegressor_select]]
	#    criterion = mse
	#    max_depth = 10
	#    max_leaf_nodes = 200
	#    min_samples_leaf = 1
	#    min_samples_split = 2
	#    n_estimators = 10
	#[[MLPRegressor]]
    #    #hidden_layer_sizes = 50, 4
    #    hidden_layer_sizes = 296, 26
    #    activation = relu
    #    solver = adam
    #    alpha = 0.001
    #    batch_size = 20
    #    learning_rate = constant

[PlotSettings]
    #feature_learning_curve = False
    #data_learning_curve = False
    target_histogram = True
    train_test_plots = True
    predicted_vs_true = True
    predicted_vs_true_bars = True
    best_worst_per_point = True
    feature_vs_target = False
    average_normalized_errors = True