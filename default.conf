[Data Setup]

    data_path = test_data.csv
    save_path = ./output
    #lwr_data_path = CD_LWR_clean8.csv
    #X = N(Cu), N(Ni), N(Mn), N(P), N(Si), N( C ), N(log(fluence), N(log(flux), N(Temp)
    X = x
    y = sin(x)

    weights = True # whether or not the weights column in the dataset is applied which duplicates weighted data

[Models and Tests to Run]

    model = gkrr_model # The name of the program that creates the model, there should be a get() function in this file which returns the model
    test_cases = ErrorBias, DescriptorImportance, KFold_CV, LeaveOutAlloyCV, FullFit, FluenceFluxExtrapolation

# AllTests section no longer needed
#[AllTests]  The configuration for AllTests.py
#
#    data_path = ${default:data_path}
#    save_path = ${default:save_path}
#    lwr_data_path = ${default:lwr_data_path}
#    weights = ${default:weights}
#    X = ${default:X}
#    Y = ${default:Y}
#    model = linear_model
#    #list of all the tests you need, name should be exactly same as the file name.
#    #The execute() function of each file will be called
#    test_cases = ${default:test_cases}


#if some test files have different configuration setting than AllTests, you can make changes by adding a
#separate section
[LeaveOutAlloyCV]
save_path = ../DBTT/graphs/leaveoutAlloy/{}.png

[Model Parameters]

    [[dtr_model]]
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    split criterion = mse

    [[gkrr_model]]
    alpha = 0.00139
    coef0 = 1
    degree = 3
    gamma = 0.518
    kernel = rbf

    [[lkrr_model]]
    alpha = 0.00518
    gamma = 0.518
    kernel = laplacian

    [[randomforest_model]]
    estimators = 100
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = None
    jobs = 1

    [[adaboost_model]]
    estimators = 275
    max_depth = 12
    min_samples_split = 2
    min_samples_leaf = 1
    learning rate = 1
    loss function = linear

    #minmax, size, transfer_function are the verbatim arguments for neurolab.net.newff()
    #training_algorithm is the verbatim 'support train fcn' for neurolab.train omitting 'train_'
    #see: https://pythonhosted.org/neurolab/lib.html#module-neurolab.net
    #epochs,show,goal are neurolab.net.train() arguments
    #see: https://pythonhosted.org/neurolab/lib.html#train-algorithms-based-gradients-algorithms
    #NOTE: minmax is verbose b/c [[0,1]]*9 will have bad pointers
    [[nn_model]]
    #minmax = [[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1]]
    minmax = [0, 1], [0, 1], [0, 1]
    size = 11, 1
    transfer_function = TanSig
    training_algorithm = bfgs
    epochs = 5
    show = False
    goal = 0.01